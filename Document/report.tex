%% LyX 2.0.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twocolumn,english]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=1,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={Your Title},
 pdfauthor={Your Name},
 pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false}
\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% for subfigures/subtables
\ifCLASSOPTIONcompsoc
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfig}
\fi

\makeatother

\begin{document}





\title{Implementation and Analysis of Parallel Shortest Path Graph Algorithms}


\author{Brandon Autrey, Anthony Bolton, Ryan Harrigan}
\maketitle
\begin{abstract}
Finding the shortest path is very important in many applications.
This is computed by graph algorithms which can be computationally
intensive on large graphs or datasets. Computational speed is important
for real-time applications such as pathing for games, traffic analysis
for GPS systems, and geodetics. In this paper we explore the parallelization
of A{*}, Dijkstra, and Bellman-Ford using OpenMP on an SMP architecture.
The performance of these algorithms are compared against their serial
versions to show their parallelization efficiency. These algorithms
are then experimented on real datasets and generated datasets to show
the shortest path. \end{abstract}
\begin{IEEEkeywords}
OpenMP, SMP, graph algorithms, shortest path, multithreading, Dijkstra\textquoteright{}s,
A{*}, Bellman-Ford, pathfinding, genetic algorithm
\end{IEEEkeywords}

\section{Introduction}

\IEEEPARstart{T}{here} are many ways for graphs to be used. Depending
on the purpose of the program, the same data can offer different insights
for the user. As there are various ways to represent data, a slurry
of algorithms are available for processing. This is interesting because
the effectiveness of algorithms are dependent on data and vice versa. 

Determining information from graphs is usually accomplished by one
of two general approaches. Read-oriented processing usually does not
alter vertices/edges. They usually run faster because writing values
pauses the traversing process. Path processing is different because
a separate memory-space is required to compute some values for decision-making.
Many algorithms have been imagined to traverse graphs for different
reasons. Such algorithms include Dijkstra\textquoteright{}s, A{*},
Prim\textquoteright{}s, and Floyd-Warshall\textquoteright{}s. These
algorithms were designed for a serial environment. When introduced
to parallelism, many difficulties arise.

Read-oriented processing is fair because it usually splits the graph
into equal parts for the cores to compute operations in parallel.
Read-oriented processing generally does not have contention when performing
operations from vertex/edge values because the outputs of threads
do not conflict with other thread\textquoteright{}s input. Shortest
path processing, however, introduces most of the difficulty. Threads
must share read/write access to shared nodes/edges and their synchronization
tends to be serial in nature. Especially in dense graphs, shortest
path processing has contention when updating adjacency lists. Both
approaches have a variety of applications, but the prevalent concerns
are focused on defining areas of possible contention when considered
for parallelism.

Parallelizing graph search algorithms is challenging. Challenges are
often addressed by the irregularity of data, locality of data in terms
of the shared global data among threads, and the partitioning of data
among threads. Irregular data causes latency for datum identification.
Data locality is difficult to predict, especially when predefined
code is absent, because locality often requires computing relationship
among many other nodes/edges. Partitioning of data computation requires
careful definitions of memory access. Determining the grain of parallelism
and boundaries of dependent node/edge values tend to make programming
complex because of these general challenges.

This paper will juxtapose serial and parallel implementations of Dijkstra\textquoteright{}s,
A{*}, Floyd-Warshal\textquoteright{}s, and genetic algorithms. These
algorithms have unique properties in parallel environments and can
improve many real-world applications. To demonstrate of these algorithms,
this paper will experiment on graph generation for games, road maps
of the United States, and LiDAR terrain data. These datasets also
have different properties and such will show various runtimes.

Implementations in this paper will often use OpenMP. OpenMP is a set
of commands that was intended to ease parallelization. Just like POSIX
threads, it is formatted to work on SMP machines, however, it can
be incorporated in distributed environments. For the sake of brevity,
implementations of algorithms will be limited to SMP environments.

OpenMP takes advantage of global memory available to the computing
nodes; allowing each node to be updated as shared memory changes.
An OpenMP application begins with the master thread. When the program
encounters a parallel region construct, OpenMP automatically creates
and distributes threads to computing nodes, overhead requiring time
before parallel computation begins. This time must be considered for
time performance and is strongly suggested that the parallel sections
have large demand \cite{citutor}. At the end of the parallel region,
the created threads are stopped and the master thread continues running.
OpenMP specifies parallel regions by pragmas, a directive based standard,
so that the source code combines serial and parallel code. The program
be compiled with or without the /openmp compiler option. If it is
compiled without it, then the pragmas are ignored and the program
behaves as entirely serial. 


\section{Related Work}

Previously an attempt to parallelize Dijkstra\textquoteright{}s using
OpenMP gained a 10\% speedup by making it parallel. This algorithm
is a difficult to parallelize to gain a significant speedup because
lots of time is being spent on parallelization and synchronization
than is actually being spent on the execution of code. The algorithm
is not suited for parallelism because it relies on priority queues
\cite{Jasika-2012-dijkstra}. Since the priority queue is the bottleneck
in the parallelization of this algorithm, this paper will attempt
an implementation of parallelizing the priority queue. 

Without even parallelizing the code, there are some optimizations
that can be made to the A{*} algorithm. For example, research has
been done discussing the use of precomputed paths for commonly used
paths, or using particular landmarks and precalculating paths between
them. Additionally, A{*} can be modified to be a bidirectional search,
wherein two searches are conducted \textquotedblleft{}simultaneously\textquotedblright{}
starting at both the origin node and the target node, branching out
until they meet in the middle. The word \textquotedblleft{}simultaneously\textquotedblright{}
is in quotes because while it may appear like the bidirectional search
is happening concurrently, with only one thread and/or processor the
calculations are still happening serially, alternating between calculating
the path from the start node and the path from the target node. Andrew
Goldberg, principal researcher at Microsoft Research Silicon Valley,
along with other colleagues have published several papers on improving
various shortest path algorithms \cite{Goldberg-2005-microsoftTechReportA*}.
They have used data sets like the North American road network (which
is almost 30 million nodes) so it is clear why it is important that
they optimize the time efficiency of their shortest path algorithms.
They have shown a variety of algorithms traversing this network with
randomly selected \textquotedblleft{}origin\textquotedblright{} and
\textquotedblleft{}destination\textquotedblright{} nodes, and it shows
the variety of routes by which these algorithms traverse the network.
Clearly, some of them are not very efficient because they visit nodes
that do not need to be considered in the first place. The image below
is that of a bidirectional search performed with Dijkstra\textquoteright{}s
algorithm with the green area emanating from the origin node and the
blue area emanating from the target node \cite{Chang-2009-shortestPath}.
Once they find a meeting point, the path has been calculated and the
search stops. 

\includegraphics[scale=0.8]{\string"D:/Dropbox/Parallel Programming Project/Anthony/Images for Reports-Presentation/bidirectionaldijkstra\string".eps}

The A{*} algorithm is essentially Dijkstra\textquoteright{}s algorithm
with an admissible heuristic (also called an optimistic heuristic)
for searching, which means it never overestimates the cost of reaching
the goal - it\textquoteright{}s always an underestimate of the cost
or the exact cost (this paper explores the Euclidean distance as the
cost) \cite{Eranki-2002-Astar}. The following image is the same network
but now with a bidirectional A{*} algorithm performed along with precalculated
landmarks \cite{Eranki-2002-Astar}.

\includegraphics[clip,scale=0.8]{\string"D:/Dropbox/Parallel Programming Project/Anthony/Images for Reports-Presentation/astar-landmarks\string".eps}

Though this implementation of the A{*} algorithm also involves precalculated
landmarks, the reader can see that a great deal of computation and
\textquotedblleft{}unnecessary nodes\textquotedblright{} are ignored
when using this P2P (point-to-point) search algorithm versus a simple
Dijkstra\textquoteright{}s.


\section{Parallel Algorithm Analysis}


\subsection{Parallel Dijkstra Algorithm}

The Dijkstra\textquoteright{}s algorithm is a graph search algorithm
that involves finding the single-source shortest path with a graph
of non-negative weights. It is widely used in applications where the
destination node is not necessary known before the algorithms starts.
An application is for finding routes within networks. This algorithm
keeps two sets, the set of nodes it has already checked and the set
of nodes that it has not checked. When searching the algorithm searches
every neighboring vertex from the previously checked vertex. This
results in checking every vertex in a circle almost, until the destination
vertex is found. The approach used was to parallelize Dijkstra was
to distribute the threads along the current source node to find the
closest neighboring node with the least weight value. The least weight
is then distributed to to the other nodes and then the source node
is updated to be the least weight. This is continued until the destination
node is found or until all the nodes in the graph are searched. 


\subsection{Parallel A{*} Algorithm}

The A{*} algorithm is a popular pathfinding algorithm used to find
the shortest distance between two points and is actually a modification
of Dijkstra's algorithm. The A{*} algorithm uses a heuristic based
on estimating the distance from any searched node to the desired target
node, and the A{*} algorithm can be made to be Dijkstra's algorithm
simply by setting the heuristic cost to zero for all the nodes. The
A{*} algorithm requires a priority queue for the optimal choices currently
held to be possible paths from the origin node to the target node,
and when one path 'deeper' in the queue is discovered to be shorter
than the shortest path so far, it climbs to the head of the priority
queue. Shortest paths algorithms like A{*} are types of combinatorial
optimization problems that can be useful for real-world applications
like route planning for shipping paths. Using multiple threads and
altering the algorithm to work more efficiently in a parallel environment,
we will show that the parallelized version of the A{*} algorithm is
more efficient and can be used to compute the shortest path very quickly
for large sets of data. For efficiency of runtime, it is important
that your graph be connected (i.e., that all your nodes are connected
to one another by way of some path in the graph and that no nodes
are isolated) or that at least your start node and your end node are
part of the same \textquotedbl{}connected component\textquotedbl{}
of the graph. If your start node and end node are NOT part of the
same connected component, your search (bidirectional or otherwise)
will search through the ENTIRE search space within the connected component(s)
of the graph that has your starting node (and within the full connected
component that has your ending node, if doing a bidirectional search)
- a very costly operation. The pseudocode for the serial version implementation
of the A{*} algorithm used in this paper is as follows:

For the purposes of parallelization, it will be necessary to create
a concurrent priority queue data structure, as well as a parallelized
dynamic array (Java was utilized, so for the serial version of the
algorithm, ArrayList was used for this purpose). A trivial way to
parallelize this algorithm a bit would involve parallelizing a bidirectional
search involving only two processors and two threads, initially. One
thread will be responsible with searching starting from the origin
node, and the other thread will be responsible for searching starting
with the destination node. When one of the threads has a path that
exceeds half the heuristic distance to the other thread\textquoteright{}s
origin node, it will begin to check whether the nodes it\textquoteright{}s
adding to the path have also been added to the other thread\textquoteright{}s
path.

Additionally, the for-loop for searching a node\textquoteright{}s
neighbors can be parallelized easily in OpenMP with their pragma-for-loop
abilities, so it will take the threads available to your computer
and put them to work in the for-loop. In general, the work of the
for-loop can be divided amongst individual threads waiting to perform
calculation-work in a thread pool, and then when their calculation
is complete, they can return to the thread pool, awaiting being used
for the next for-loop of either of the bidirectional searches. Additionally,
there is the possibility of having a separate thread go through the
combined set of nodes included so-far in the two paths (since a bidirectional
search will be performed) to stop the searches once it encounters
a node that states it is in both thread\textquoteright{}s paths. It
is the attempt of this paper to show that these changes to the algorithm
will result in an optimized runtime of the algorithm on large datasets.


\subsection{Parallel Bellman-Ford Algorithm}


\section{Experimentation and Testing}

A paper written by Luis Henrique Oliveira Rios and Luiz Chaimowicz
of the Department of Computer Science of Federal University of Minas
Gerais in Brazil suggests a method of parallelizing the A Star search
algorithm that uses a bidirectional search, much like our proposed
method. Their version, called Parallel New Bidirectional A{*} \cite{riospnba},
proposes that rather than having an 'open Nodes' priority queue that
contains the nodes on the frontier, each of two threads (one starting
at the goal toward the starting node and the other from the starting
node to the goal node) contains its own open Nodes priority queue.
Additionally, each thread maintains its own F, G, and H function values
for its nodes, but instead of holding an array for \textquoteleft{}closed
Nodes\textquoteright{} that holds the Nodes that have already been
expanded, the two threads have shared access to a set of nodes, M,
that contains all the Nodes that are \textquotedblleft{}in the middle\textquotedblright{}.
Initially, all of the nodes are placed in this set, M, and as they
are expanded (meaning, their neighbors are visited and exhausted),
they are removed from the set M. L is also a variable that is shared
by both threads - it is also read and written by both threads. It
represents the cost of the best solution found by the algorithm so
far and is initialized with the value of infinity. The source code
available in the paper which was cited is represented by the pseudocode
pictured below. In this other paper, the group used the C++ programming
language along with the pthreads library. 

\includegraphics[scale=0.8]{\string"D:/Dropbox/Parallel Programming Project/PNBA\string".eps}

In this paper\textquoteright{}s implementation, the Java programming
language was used with its own natural threading capability. Two threads
were created much like the pseudocode above suggests, with all subscripts
(\textquoteleft{}1s\textquoteright{} and \textquoteleft{}2s\textquoteright{})
switching (becoming \textquoteleft{}2s\textquoteright{} and \textquoteleft{}1s\textquoteright{})
for the opposite thread (thread 1 starting from the start node and
seeking the goal node, and thread 2 starting from the goal node and
seeking the start node). 

The termination condition is that there are no more Nodes in the shared
M set of \textquotedblleft{}middle Nodes\textquotedblright{} - this
is admissible because even if a node is searched twice by both threads,
the search is guaranteed to choose the Nodes that lead to the smallest
L value. 

Unfortunately, the implementations of the above pseudocode proved
to perform worse than their serial, unidirectional counterparts. This
may be due to the overhead of creating the Threads initially, or locking
the critical section with a Semaphore. Or, this may have had to do
with the fact that the shared variables M and L were being accessed
by the threads by a much larger component that was passed into the
Threads as arguments, and was set to volatile. Such a large component
(the GraphGUI class itself) with so many intricate, threaded parts
(it also handled the GUI creation, so painting and repainting the
screen) probably should not have been treated as volatile. Most likely
the speed decrease was due to the reasons above, but otherwise it
seems, logically, that the implementation of the pseudocode above
should have been sound otherwise. The next several images will showcase
the decrease in speed, as opposed to the expected increase that parallelizing
the A{*} algorithm should have produced.

\includegraphics[scale=0.3]{\string"D:/Dropbox/Parallel Programming Project/serialAStar\string".eps}

As the reader will note, the A{*} search started in the middle of
the graph and the goal node was in the bottom right corner. The red
lines represent failed paths, whereas the black line represents the
actual shortest path computed. At the bottom of the GUI window note
that the execution time (of the actual A{*} algorithm itself, and
not in drawing to the GUI) is approximately 0.4477 seconds. The equivalent
parallelized A{*} search with the same starting and ending Node is
pictured below. 

The reader will now note that the search space has increased slightly
(because the goal node is now seeking the start node as well, so it
searches around itself), and the path is visibly incorrect. The process
for drawing the path became disjoint, but the portion going from the
goal node to the start node is correct. 

\includegraphics[scale=0.3]{\string"D:/Dropbox/Parallel Programming Project/parallelAStar\string".eps}

Also of interest is that the execution time (simply for A{*} and not
for drawing the GUI), is now 2.9745 seconds, and in the console window
you can see the time taken by BOTH threads is greater than the time
the single, unidirectional A{*} search endured. 

\includegraphics[scale=0.3]{\string"D:/Dropbox/Parallel Programming Project/serialDijkstra\string".eps}

Recall that A{*} search becomes equivalent to Dijkstra\textquoteright{}s
search algorithm when the heuristic is set to 0. Above you can not
that Dijkstra\textquoteright{}s takes approximately .7 seconds when
executed serially. However, as the picture below will attest, it took
almost one order of magnitude longer (4.39 seconds) for the two threads
to execute. 

\includegraphics[scale=0.3]{\string"D:/Dropbox/Parallel Programming Project/ParallelDijkstra\string".eps}

\bibliographystyle{IEEEtran}
\bibliography{report_ref}

\end{document}
